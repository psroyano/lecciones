{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lección 4. Clasificador Bayesiano\n",
    "\n",
    "Ejecuta la primera celda de código si no está visible la presentación y sigue la presentación en Genially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"https://view.genial.ly/62bb63db13e30f00172e1882\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x243c38611c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.display.IFrame('https://view.genial.ly/62bb63db13e30f00172e1882',900,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos\n",
    "\n",
    "Disponemos de una serie de ejemplos de entrenamiento (instancias) definidos en base a unos atributos y una clase asignada. Pretendemos construir un modelo que nos sirva para clasificar nuevas instancias. \n",
    "\n",
    "En este ejemplo disponemos de 6 instancias relacionadas con compras por internet. Los atributos son:\n",
    "+ A1: Sitio de acceso. Atributo discreto, valores posibles: Web, App\n",
    "+ A2: Cantidad gastada. \n",
    "\n",
    "Las clases posibles dividirán a los compradores en dos grupos:\n",
    "+ Bueno\n",
    "+ Malo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador Naive Bayes\n",
    "Gracias al teorema de Bayes, dado un conjunto de ejemplos, podemos crear un clasificador, de manera que podamos clasificar nuevas instancias en la clase más probable. Vamos a ejecutar el algoritmo de clasificación NaiveBayes de forma manual. Primero definimos cuáles son las entradas y salidas del algoritmo. \n",
    "\n",
    "### Entradas\n",
    "+ A: conjunto de atributos\n",
    "+ V: conjunto de posibles valores de los atributos\n",
    "+ C: conjunto de clases\n",
    "+ E: conjunto de instancias de entrenamiento\n",
    "\n",
    "### Salidas\n",
    "+ $M_C$: matriz de probabilidades a priori de clases.\n",
    "+ $M_{AD}$: matriz de probabilidades condicionales de atributos discretos\n",
    "+ $M_{AC}$: matriz de probabilidades condicionales de atributos continuos\n",
    "\n",
    "### Uso del clasificador\n",
    "Una vez obtenidas las matrices de salida, podemos clasificar nuevas instancias gracias al teorema de Bayes.\n",
    "\n",
    "$ p(C_k | X_i) = \\frac { p(X_i | C_k)  p(C_k) } {p(X_i)} $\n",
    "\n",
    "Siendo:\n",
    "+ $p(C_k | X_i)$ la probabilidad de que un ejemplo $X_i$ pertenezca a la clase $C_k$\n",
    "+ $p(X_i | C_k)$ la probabilidad de que un ejemplo cualquiera de una clase $C_k$ sea $X_i$\n",
    "+ $p(C_k)$ la probabilidad de que un ejemplo sea de la clase $C_k$\n",
    "+ $p(X_i)$ la probabilidad de que exista el ejemplo $X_i$\n",
    "\n",
    "La clase de la nueva instancia corresponde a la clase que maximiza la probabilidad según el teorema de Bayes. Dado que el denominador es común para todas las clases, no es necesario ejecutar la división. Así, la clase \"c\" será aquella $C_k$ que maximice\n",
    "\n",
    "$c = \\arg \\max p(X_i | C_k)  p(C_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo manual de aplicación Naive Bayes\n",
    "\n",
    "El algoritmo es más simple de entender mediante un ejemplo, así que vamos a ejecutar el algoritmo de manera manual, de manera que procederemos a calcular las matrices haciendo uso de sentencias simples de Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidades de clase a priori, matriz $M_c$\n",
    "\n",
    "La probabilidad a priori de que un ejemplo sea de clase j, p(clase_j) = número de instancias clase_j / número total instancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad a priori de que un ejemplo sea de clase bueno =  0.6666666666666666\n",
      "Probabilidad a priori de que un ejemplo sea de clase malo =  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Cálculo de matriz de probabilidades a priori de clases Mc\n",
    "\n",
    "# Las instancias totales del conjunto de entrenamiento son 6\n",
    "total_instancias = 6\n",
    "\n",
    "# El número de instancias de la clase bueno son 4\n",
    "instancias_bueno = 4\n",
    "# El número de instancias d ela clase malo son 2\n",
    "instancias_malo = 2\n",
    "\n",
    "# Calculamos las probabilidades a priori \n",
    "p_bueno = instancias_bueno / total_instancias\n",
    "p_malo = instancias_malo / total_instancias \n",
    "\n",
    "print(\"Probabilidad a priori de que un ejemplo sea de clase bueno = \", p_bueno)\n",
    "print(\"Probabilidad a priori de que un ejemplo sea de clase malo = \", p_malo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidades condicionales de atributos discretos, matriz $M_{AD}$\n",
    "\n",
    "### $p(SitioAcceso | Clase)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(Sitio_acceso | clase)\n",
      "Sitio acceso      Bueno    Malo\n",
      "--------------  -------  ------\n",
      "Web                0.25     0.5\n",
      "App                0.75     0.5\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Cálculo de matriz de probabilidades condicionales de atributo discreto Sitio Acceso\n",
    "\n",
    "# p(Sitio_acceso=0 | C=bueno) = Ejemplos Sitio_acceso=0 y Clase=bueno / Ejemplos clase=bueno\n",
    "p_SAW_Cbueno = 1 / instancias_bueno\n",
    "p_SAA_Cbueno = 3 / instancias_bueno\n",
    "\n",
    "\n",
    "\n",
    "# p(Sitio_acceso=0 | C=bueno) = Ejemplos Sitio_acceso=0 y Clase=bueno / Ejemplos clase=bueno\n",
    "p_SAW_Cmalo = 1 / instancias_malo \n",
    "p_SAA_Cmalo = 1 / instancias_malo \n",
    "\n",
    "\n",
    "# Aquí simplemente guardamos los datos en una matriz para mostrarla con la función \"tabulate\" de forma más elegante\n",
    "discreto = [[\"Web\", p_SAW_Cbueno, p_SAW_Cmalo], \n",
    "     [\"App\", p_SAA_Cbueno, p_SAA_Cmalo]]\n",
    "print(\"p(Sitio_acceso | clase)\")\n",
    "print(tabulate(discreto, headers=[\"Sitio acceso\", \"Bueno\", \"Malo\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidades condicionales de atributos continuos\n",
    "Para atributos continuos, vamos a calcular la media y la varianza del atributo por cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atributo continuo Cantidad\n",
      "            Bueno    Malo\n",
      "--------  -------  ------\n",
      "media         150      75\n",
      "varianza     6950     625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "cantidades_bueno = [120, 200, 30, 250]\n",
    "media_bueno = np.mean(cantidades_bueno)\n",
    "varianza_bueno = np.var(cantidades_bueno)\n",
    "\n",
    "\n",
    "\n",
    "cantidades_malo = [100, 50]\n",
    "media_malo = np.mean(cantidades_malo)\n",
    "varianza_malo = np.var(cantidades_malo)\n",
    "\n",
    "\n",
    "# Aquí simplemente guardamos los datos en una matriz para mostrarla con la función \"tabulate\" de forma más elegante\n",
    "continuo = [[\"media\", media_bueno, media_malo], \n",
    "     [\"varianza\", varianza_bueno, varianza_malo]]\n",
    "print(\"Atributo continuo Cantidad\")\n",
    "print(tabulate(continuo, headers=[\"\", \"Bueno\", \"Malo\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso del conocimiento \n",
    "\n",
    "Ya tenemos calculados los 3 elementos basados en los datos de entrenamiento, que son:\n",
    "+ Probabilidades a priori de cada clase\n",
    "+ Probabilidades condicionales de los atributos discretos\n",
    "+ Medias y varianzas de atributos continuos\n",
    "\n",
    "Estamos preparados para clasificar nuevas instancias en la clase más probable. Para ello, aplicamos la formula al nuevo ejemplo, para cada clase k:\n",
    "\n",
    "$c = \\arg \\max p(X_i | C_k)  p(C_k)$. Siendo :\n",
    "+ $p(C_k)$ : La probabilidad a priori de que el ejemplo sea de clase k. \n",
    "\n",
    "+ $p(X_i | C_k)$:  la multiplicación de las probabilidades condicionales de cada atributo. \n",
    "    + Si el atributo es discreto: La probabilidad la tenemos calculada en la matriz $M_{AD}$\n",
    "    + Si el atributo es continuo, se asume una distribución normal. En base a la media y varianza calculadas anteriormente. \n",
    "    $p(x)= \\frac {1}{\\sqrt{2\\pi\\sigma^2}} e ^ -\\frac {1}{2} \\frac {(x-\\mu)^2}{\\sigma^2}  $\n",
    "\n",
    "\n",
    "Debemos calcular la probabilidad del nuevo ejemplo para cada clase, siendo la clase resultante la que mayor probabilidad tenga.\n",
    "\n",
    "El  nuevo ejemplo a clasificar tiene los siguientes valores en los atributos estudiados:\n",
    "+ Cantidad: 300\n",
    "+ Sitio: App\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilidad de que el nuevo ejemplo sea de clase Bueno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0023670151452974287\n"
     ]
    }
   ],
   "source": [
    "# ATRIBUTO DISCRETO\n",
    "# p(SiticioAcceso = Web | Clase = Bueno ), ya tenemos calculadas las probabilidades\n",
    "p_SAA_CB = 0.75\n",
    "\n",
    "# ATRIBUTO CONTINUO\n",
    "# Para atributos continuos, calculamos la probabilidad asumiendo distribución normal\n",
    "media_bueno = 150\n",
    "varianza_bueno = 6950\n",
    "x = 300\n",
    "\n",
    "# Calculamos la raiz cuadrada de (2 * pi * varianza) utilizando NumPy.sqrt() y NumPy.pi \n",
    "raiz = np.sqrt(2* np.pi *varianza_bueno)\n",
    "\n",
    "# Calculamos el exponente \n",
    "exponente = -(1/2) * ((x-media_bueno) / varianza_bueno)\n",
    "\n",
    "# Calculamos la probabilidad. Con la función NumPy.exp(número) para calcular la potencia con base número e\n",
    "p_x_CB = (1 / raiz) * np.exp(exponente)\n",
    "\n",
    "# Multiplicamos las probabilidades condicionadas de cada atributo y la probabilidad a priori\n",
    "p_ejemplo_CB = p_SAA_CB * p_x_CB * p_bueno\n",
    "\n",
    "print(p_ejemplo_CB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilidad de que el nuevo ejemplo sea de clase Malo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002221497352611997\n"
     ]
    }
   ],
   "source": [
    "# ATRIBUTO DISCRETO\n",
    "# p(SiticioAcceso = Web | Clase = Bueno ), ya tenemos calculadas las probabilidades\n",
    "p_SAA_CM = 0.5\n",
    "\n",
    "# ATRIBUTO CONTINUO\n",
    "# Para atributos continuos, calculamos la probabilidad asumiendo distribución normal\n",
    "media_malo = 75\n",
    "varianza_malo = 625\n",
    "x = 300\n",
    "\n",
    "# Calculamos la raiz cuadrada de (2 * pi * varianza) utilizando NumPy.sqrt() y NumPy.pi \n",
    "raiz = np.sqrt(2* np.pi *varianza_malo)\n",
    "\n",
    "# Calculamos el exponente \n",
    "exponente = -(1/2) * ((x-media_malo) / varianza_malo)\n",
    "\n",
    "# Calculamos la probabilidad. Con la función NumPy.exp(número) para calcular la potencia con base número e\n",
    "p_x_CM = (1 / raiz) * np.exp(exponente)\n",
    "\n",
    "# Multiplicamos las probabilidades condicionadas de cada atributo y la probabilidad a priori\n",
    "p_ejemplo_CM = p_SAA_CM * p_x_CM * p_malo\n",
    "\n",
    "print(p_ejemplo_CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Una vez hechos los cálculos, comprobamos el máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El nuevo ejemplo queda clasificado en la clase Bueno\n"
     ]
    }
   ],
   "source": [
    "if (p_ejemplo_CB > p_ejemplo_CM):\n",
    "    print(\"El nuevo ejemplo queda clasificado en la clase Bueno\")\n",
    "else:\n",
    "    print(\"El nuevo ejemplo queda clasificado en la clase malo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusiones\n",
    "\n",
    "Hemos demostrado como a través del cálculo de probabilidades, medidas estadísticas y Teorema de Bayes, podemos entrenar un modelo para clasificar nuevas instancias. Este ejemplo ha sido lo más sencillo posible para demostrar los fundamentos del modelo. Gracias a las bibliotecas de Python podemos realizar problemas más complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes con Scikit-Learn\n",
    "\n",
    "Scikit Learn es una biblioteca con funciones predefinidas orientadas a la creación de modelos de aprendizaje automático. Uno de estos modelos es \"GaussianNB\", con el mismo fundamento como el que acabamos de hacer manualmente, pero siendo todos los atributos de tipo numérico. \n",
    "\n",
    "Para esta ocasión, usaremos un dataset más práctico y muy conocido en aprendizaje automático, clasificación de flores por tipo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases:  ['setosa' 'versicolor' 'virginica']\n",
      "Atributos: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "Descripción del dataset:\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# Importamos el dataset, que pertenece a la biblioteca Scikit-Learn\n",
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "\n",
    "# INFORMACIÓN DEL DATASET\n",
    "# Los nombres de las clases están almacenadas en target_names\n",
    "print(\"Clases: \", dataset.target_names) \n",
    "\n",
    "# Los nombres de los atributos están almacenados en feature_names\n",
    "print(\"Atributos:\", dataset.feature_names) \n",
    "\n",
    "# Podemos mostrar una descripción completa del conjunto de datos\n",
    "print(\"\\nDescripción del dataset:\")\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División del dataset, necesidad de valdidar el modelo.\n",
    "\n",
    "Si entrenamos a nuestro modelo con todos los datos disponibles resultaría difícil saber cómo de eficaz es a la hora de clasificar nuevas instancias. En este tipo de modelos podemos dividir el dataset en dos conjuntos, una parte de los datos servirá para entrenar al modelo, mientras que el segundo conjunto servirá para validarlo. Dado que ya conocemos de antemano a qué clase pertenecen las instancias del conjunto de validación, podemos comparar la clase predicha por el modelo con la real y calcular cuantos errores ha cometido en su tarea de clasificación. \n",
    "\n",
    "Scikit-Learn dispone de funciones para dividir el dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Para dividir el dataset usamos funciones predefinidas de la biblioteca Scikit-Learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Almacenamos el dataset en dos variables. \n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# En la Matriz X el conjunto de atributos. Tamaño 4x150, 4 características y 150 instancias. \n",
    "print(X)\n",
    "\n",
    "# Vector y, tamaño 150, almacena a qué clase pertenece cada instancia (numeradas de 0 a 2)\n",
    "print(y)\n",
    "\n",
    "# Dividimos los datos en 2 conjuntos del mismo tamaño, al 50%, datos de entrenamiento y datos de test. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y validación del modelo.\n",
    "\n",
    "Con la biblioteca, todos los cálculos realizados manualmente al inicio de la lección pasan a convertirse en una línea de código. \n",
    "\n",
    "Además de entrenar al modelo, vamos a validarlo con el conjunto de test y a comparar los resultados predichos con la clase real y ver el número de fallos cometidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias clasificadas 75, número de errores: 4\n"
     ]
    }
   ],
   "source": [
    "# Importamos el modelo GaussianNB, perteneciente al paquete naive bayes de Scikit-Learn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "modeloNB = GaussianNB()\n",
    "\n",
    "# Entrenamos el modelo pasándole los datos de entrenamiento como parámetros\n",
    "modeloNB.fit(X_train, y_train)\n",
    "\n",
    "# Para validar el modelo, vamos a predecir la clase de los datos del conjunto de test\n",
    "y_pred = modeloNB.predict(X_test)\n",
    "\n",
    "# Comparamos la clase predicha con la real y mostramos el número de errores\n",
    "print(\"Instancias clasificadas %d, número de errores: %d\"% (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusión\n",
    "\n",
    "La matriz de confusión es una herramienta para visualizar cómo de válido es el modelo, mostrando en forma de matriz el número de instancias con los aciertos y fallos correspondientes.\n",
    "\n",
    "En nuestro conjunto de test, disponíamos de 75 instancias:\n",
    "+ 21 instancias de clase 0, clasificadas correctamente.\n",
    "+ 30 instancias de clase 1, clasificadas correctamente.\n",
    "+ 24 instancias de clase 2, de ellas, 20 clasificadas correctamente y 4 clasificadas como clase 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21  0  0]\n",
      " [ 0 30  0]\n",
      " [ 0  4 20]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Mostramos por pantalla la matriz de confusión\n",
    "#Pasamos como parámetros los valores reales del conjunto de test y la predicción del modelo\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para practicar y evaluar lo aprendido\n",
    "\n",
    "Visualiza la presentación (ejecuta la celda si no está visible) y responde al Quiz ayudándote de la programación en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"https://view.genial.ly/62bb640713e30f00172e18a5\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x22754e5eca0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.display.IFrame('https://view.genial.ly/62bb640713e30f00172e18a5',900,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el dataset, que pertenece a la biblioteca Scikit-Learn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "# Almacenamos el dataset en dos variables. \n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Sigue el programa para responder a las preguntas formuladas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
